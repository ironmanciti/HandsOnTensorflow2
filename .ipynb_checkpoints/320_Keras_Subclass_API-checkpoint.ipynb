{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wF5wszaj97Y"
   },
   "source": [
    "# 전문가를 위한 Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "[MNIST 데이터셋](http://yann.lecun.com/exdb/mnist/)을 로드하여 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 이를 0.0에서 1.0 사이의 값으로 정규화하여 모델이 더 잘 학습할 수 있도록 합니다.\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0 \n",
    "\n",
    "# 이미지 데이터는 보통 (높이, 너비, 채널)의 3차원 텐서로 표현됩니다. \n",
    "# mnist 데이터셋은 흑백 이미지이므로 채널이 1개이지만, 이 정보가 누락되어 2차원 텐서(높이, 너비)로 제공됩니다. \n",
    "# 따라서 채널 차원을 추가하여 데이터를 올바른 형태로 만듭니다.\n",
    "X_train = X_train[..., tf.newaxis].astype(\"float32\") \n",
    "X_test = X_test[..., tf.newaxis].astype(\"float32\") \n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1Evqx0S22r_"
   },
   "source": [
    "tf.data를 사용하여 데이터셋을 섞고 배치를 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train과 해당 레이블 y_train을 바탕으로 Dataset 객체를 생성\n",
    "# 이 Dataset 객체는 텐서를 슬라이스하여 각 요소를 개별적으로 처리할 수 있게 해줍니다.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \n",
    "\n",
    "# 10000은 버퍼의 크기로, 이 값이 전체 데이터셋의 크기보다 크거나 같으면 전체 데이터가 매 epoch 마다 섞입니다.\n",
    "# batch() 함수는 데이터셋을 일정한 배치 크기로 분할합니다. 이 예제에서는 배치 크기를 32로 설정했습니다.\n",
    "train_ds = train_ds.shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \n",
    "test_ds = test_ds.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPZ68wASog_I"
   },
   "source": [
    "케라스(Keras)의 [모델 서브클래싱(subclassing) API](https://www.tensorflow.org/guide/keras#model_subclassing)를 사용하여 `tf.keras` 모델을 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h3IKyzTCDNGo"
   },
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# model instance 생성\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGih-c2LgbJu"
   },
   "source": [
    "훈련에 필요한 옵티마이저(optimizer)와 손실 함수를 선택합니다: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u48C9WQ774n4"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB6A1vcigsIe"
   },
   "source": [
    "모델의 손실과 성능을 측정할 지표를 선택합니다. 에포크가 진행되는 동안 수집된 측정 지표를 바탕으로 최종 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N0MqHFb4F_qn"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix4mEL65on-w"
   },
   "source": [
    "`tf.GradientTape`를 사용하여 모델을 훈련합니다:   \n",
    "\n",
    "- with tf.GradientTape() as tape: 로 저장할 tape을 지정해주면, 이후의 GradientTape() 문맥 아래에서의 TensorFlow의 연관 연산 코드는 tape에 저장이 됩니다. 이렇게 tape에 저장된 연산 과정 (함수, 연산식) 을 가져다가 TensorFlow는 dx = tape.gradient(loss, x) 로 후진 모드 자동 미분 (Reverse mode automatic differentiation) 방법으로 손실에 대한 x의 미분을 계산합니다. 이렇게 계산한 손실에 대한 x의 미분을 역전파(backpropagation)하여 x의 값을 갱신(update)하는 작업을 반복하므로써 변수 x의 답을 찾아가는 학습을 진행합니다.   \n",
    "\n",
    "\n",
    "- def 위에 @tf.fucnction annotation을 붙이면 마치 tf2.x 버전에서도 tf1.x 형태처럼 그래프 생성과 실행이 분리된 형태로 해당 함수내의 로직이 실행되게 됩니다. (실행 속도가 약간 빨라질 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function 데코레이터는 그래프 모드에서 실행하도록 함수를 최적화하고, 속도를 향상시킵니다.\n",
    "# 이 데코레이터를 사용하면 모델의 학습 속도를 높일 수 있습니다.\n",
    "@tf.function \n",
    "def train_step(images, labels):\n",
    "\n",
    "    # tf.GradientTape는 자동 미분(주어진 입력 변수에 대한 연산의 그래디언트를 계산)을 위한 API입니다. \n",
    "    # 'with' 키워드를 사용하여 연산을 기록\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # 모델을 사용해 이미지를 예측합니다. training=True는 학습 중임을 나타냅니다.\n",
    "        # Dropout과 같은 일부 레이어는 학습과 테스트 시에 다르게 동작하기 때문에 이 구분이 필요합니다.\n",
    "        predictions = model(images, training=True) \n",
    "\n",
    "        loss = loss_object(labels, predictions) \n",
    "\n",
    "    # 그래디언트를 계산합니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables) \n",
    "\n",
    "    # 계산된 그래디언트를 사용해 옵티마이저를 통해 학습 가능한 변수를 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) \n",
    "\n",
    "    # 현재 배치에 대한 손실을 학습 손실 기록에 추가합니다.\n",
    "    train_loss(loss) \n",
    "\n",
    "    # 현재 배치에 대한 정확도를 학습 정확도 기록에 추가합니다.\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8YT7UmFgpjV"
   },
   "source": [
    "모델을 테스트합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def test_step(images, labels):\n",
    "\n",
    "    # 모델을 사용하여 이미지를 예측합니다. training=False는 추론(또는 테스트) 중임을 나타냅니다.\n",
    "    # Dropout과 같은 일부 레이어는 학습과 테스트 시에 다르게 동작하기 때문에 이 구분이 필요\n",
    "    predictions = model(images, training=False) \n",
    "\n",
    "    t_loss = loss_object(labels, predictions) \n",
    "\n",
    "    # 현재 배치에 대한 손실을 테스트 손실 기록에 추가합니다.\n",
    "    test_loss(t_loss) \n",
    "\n",
    "    # 현재 배치에 대한 정확도를 테스트 정확도 기록에 추가합니다.\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0147, Accuracy: 99.5250, Test Loss: 0.0542, Test Accuracy: 98.4500\n",
      "Epoch 2, Loss: 0.0094, Accuracy: 99.6950, Test Loss: 0.0685, Test Accuracy: 98.3300\n",
      "Epoch 3, Loss: 0.0072, Accuracy: 99.7567, Test Loss: 0.0743, Test Accuracy: 98.3800\n",
      "Epoch 4, Loss: 0.0073, Accuracy: 99.7433, Test Loss: 0.0615, Test Accuracy: 98.5000\n",
      "Epoch 5, Loss: 0.0040, Accuracy: 99.8767, Test Loss: 0.0664, Test Accuracy: 98.4200\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # 각 에폭이 시작될 때 마다, 학습과 테스트의 손실 및 정확도 메트릭을 리셋\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    # 학습 데이터셋의 모든 배치에 대해 한 단계의 학습을 수행\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    # 테스트 데이터셋의 모든 배치에 대해 한 단계의 테스트를 수행\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    # 각 에폭이 끝날 때마다 학습 손실, 학습 정확도, 테스트 손실, 테스트 정확도를 출력\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, ' \n",
    "        f'Loss: {train_loss.result():.4f}, ' \n",
    "        f'Accuracy: {train_accuracy.result() * 100:.4f}, ' \n",
    "        f'Test Loss: {test_loss.result():.4f}, ' \n",
    "        f'Test Accuracy: {test_accuracy.result() * 100:.4f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "920_advanced_Keras.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
